\chapter{Background}

This chapter describes some background information on Approximate Computing and the automated framework IIDEAA. Section on Approximate Computing will cover its definition, motivation, challenges, and some basic strategies. As for IIDEAA, we will present two tools that the framework is made up of: Clang-Chimera, a source-to-source mutation software that apply pre-defined code mutator corresponding to some of the strategies. The other tool is an Evolution search engine named Bellerophon, which goes through Chimera's mutated source code to find the best approximated version of the program, based on how the user decide to calculate the error, reward and penalty functions.
\section{Approximate Computing}

\subsection{Definition}

Approximate computing is a computational technique that loosen up the accuracy requirement of mathematical operations performed by computers, returning a result that is possibly inaccurate (but not incorrect) compares to the exact result, yet is still within the range of "acceptable error" for an application to function properly. Such applications that enable Approximate Computing are called "error-tolerant" applications \cite{7348659}. These applications usually have gaps between the accuracy level it(or the user) require and that of the system can deliver, and thus is capitalized by Approximate Computing to produce a variety of optimizations \cite{AxCSurvey} \\
~\\
By reducing the stress of exact calculation and some minor loss of precision, this technique can promise a gain in efficiency, in terms of speed, memory, energy consumption, etc. As a prime example, the approximated version of k-mean clustering algorithm, tested by Chippa, achieved up to 50x energy reduction while losing only 5\% of accuracy and up to 5x energy reduction with insignificant error \cite{SEHD} .\\
~\\
Approximate Computing can be conducted on both hardware and software layer, providing diverse methods to apply the technique to applications. On hardware level, the target can be either: circuits, by replacing them with less accurate but more energy-efficient one, or the voltage of the hardware component, deliberately lowering it to make a compromise between accuracy and energy-consumption. With software layer, the general strategy is to overlook parts of the program that have insignificant contribution to the final result \cite{7348659}. \\

\subsection{Motivation}

Current era computing is, while advanced, without its own dilemma. Large-scale applications, namely scientific computing, social media, business and financial analysis drain far too much resources than what is available. A prediction was made that by 2020, datacenters in US will consume about 140 billion kWh of electricity, a huge increase from 91 billion kWh in 2013 \cite{NRDC}. It is clear that while computing applications are achieving incredible performance and results, the amount of resources they require is soon to be out of control as the planet's resources are limited and decreasing everyday \cite{AxCSurvey}. \\
~\\
One of the main cause of this problem is fault-free computing, which can be usually resource-intensive. The current generation computer circuit's design contains components are more vulnerable to faults and parameter variations due to low voltage supply and ever-growing integration density \cite{1322441}. Thus, for faultless computation to be used, guardbands for protection against parameter variations and error correction is apply, which increase the energy overhead tremendously \cite{7348659}. \\
~\\
Due to this fact, Approximate Computing raises as a potential solution, being the topic of interest in both industry and academic researches \cite{7348659}. Furthermore, many modern days application have inexact or noisy input, limited data precision, or are not able to find an exact output. Consequently, faultless computing becomes more redundant as rounding off results happens more often, making Approximate Computing a more optimal choice \cite{AxCSurvey}. \\

\subsection{Level of Approximate Computing}
In the article "Introduction to Approximate Computing", the author, Ben Khadra, has suggested a structure to classify all forms of development for Approximate Computing. One parameter of classification among the structure presented is the Level of Approximation, indicating which part of the system the technique is applied on. The four following categories were presented from lowest to highest respectively, in terms of cost-effective: algorithm, application, architecture, and circuit \cite{introAxC}. \\
%insert image here
Changes made at algorithm level usually are related to inputs or configuration of the algorithm while not altering the said algorithm. For application level, on the other hand, modifications to the algorithm are required. Another way to apply Approximate Computing at this level is having the ability to inspect a larger search space, for example automating the process to discover many Approximated variants of the application. Higher levels of Approximate Computing typically include adjustments to hardware, with the top-level being circuit modification. At this level, the design of the circuit is the target by changing or adding new arithmetic units, for instance adders and multipliers, which is created for approximate computing. This level is also considered as the most expensive to implement since implementations physically change the circuit into a new one \cite{introAxC}.
\subsection{Common Strategies}

\subsubsection{Precision Scaling}
As the name imply, this strategy focuses on alternating the precision/bit-width of the input or intermediate operations. Precision Scaling is usually used with Floating Point operations, and generally results in a reduction in memory requirement or computational stress. It has been revealed that by decreasing the bit-width, it allow some optimizations can be viable. One of them being reducing Floating Point operations to a much simpler, insignificant operations (such as multiply by 1) which in turns do not require a Floating Point Unit. Another is allowing the use of smaller, much faster Floating Point Unit than the typical Floating Point Unit \cite{4408271}. \\
~\\
While Precision Scaling is mainly applied to hardware layer, more specifically on architechtural level, it can still be used on software level. To perform Precision Scaling on hardware layer, one can target directly the Floating Point Unit, altering its architecture \cite{AxCSurvey}. However the same cannot be said for software layer. Wwithout specific hardware and programming library support, it is almost impossible to do so since programmers are stuck with very basic data type, such as single-precision (32-bit) and double-precision (64-bit) floating point number. \\
~\\
Currently, with the introduction of Pascal GPU Architecture and CUDA8 API, NVIDIA has provided half-precision (16-bit) floating point number data type for computing in lower precision \cite{CUDA8}. Despite not being as in full control nor as flexible as hardware Precision Scaling, the 16-bit floating point number has made Precision Scaling strategy more viable on software level, especially how GPU has been playing an important part in modern High Performance Computing. \\

\subsubsection{Loop Perforation}
Unlike Precision Scaling which mostly aims at hardware layer, Loop Perforation explicitly targets the software layer, performing Approximate Computing by directly making changes to the application. The strategy reduces computational time(thus reducing resources consumption) by only executing only a major portion of the loop's total iterations while skipping the some of it. Loop Perforation can succeed due to the fact it takes advantage of partially redundant computations that often manifest when processing multiple inputs to acquire one output. However, this strategy is not always feasible since there are many applications rely on hard logical correctness to function properly such as compilers, databases, operating systems \cite{LoopPerforation}. \\

\subsection{Challenges}

\section{IIDEAA Automate Framework} 

\subsection{Clang Chimera}

\subsection{Bellerophon}